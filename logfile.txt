PowerShell 7.4.2

   A new PowerShell stable release is available: v7.4.3 
   Upgrade now, or check out the release page at:       
     https://aka.ms/PowerShell-Release?tag=v7.4.3       

Loading personal and system profiles took 2579ms.
@26921 ➜ E:\..\L-Defense_EFND_RE ( base 3.11.5) git(master) 
[ 100] conda activate efnd   
@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 226ms
[ 100] ls

        Directory: E:\doc\projectPy\L-Defense_EFND_RE


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d----         2024/6/17     21:19                  __pycache__
d----         2024/6/22     18:21                  .idea
d----         2024/6/17     10:31                  conf
d----         2024/6/17     14:40                  dataset
d----         2024/6/17     21:20                  runnings
d----         2024/6/22     18:54                  source
-a---         2024/6/17     21:52            175   .gitignore
-a---         2024/6/17     22:07            340 󰉢  environment.yml
-a---         2024/6/22     17:30           8392   help.py
-a---         2024/6/17     21:59             79 󰈙  requirements.txt
-a---         2024/6/18     14:50           4148   roberta.md

@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 
[ 100] python source/extractor_train.py --model_name_or_path roberta-base --dataset_name RAWFC --output_dir ./runnings/step1_extraction_model_RAWFC --do_train --train_batch_size 1 --num_train_epochs 5 --eval_batch_size 32 --eval_steps 500 --max_seq_len 32 --gradient_accumulation_steps 1 --correlation_method mlp --learning_rate 1e-5 --cls_loss_weight 0.9
D:\04code\anaconda3\envs\efnd\lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EvidenceSelection were not initialized from the model checkpoint at roberta-base and are newly initialized: ['corre_fc1.bias', 'corre
_fc1.weight', 'corre_fc2.bias', 'corre_fc2.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight', 'fc4.bias', 'fc4.we
ight', 'predict_fc0.bias', 'predict_fc0.weight', 'predict_fc1.bias', 'predict_fc1.weight', 'predict_fc2.bias', 'predict_fc2.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1612/1612 [00:00<00:00, 25828.65it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 24599.30it/s] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 28577.39it/s] 
D:\04code\anaconda3\envs\efnd\lib\site-packages\transformers\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will 
be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/22 06:57:10 PM: ***** Running training *****
06/22 06:57:10 PM:   Num examples = 1612
06/22 06:57:10 PM:   Num Epochs = 5
06/22 06:57:10 PM:   Gradient Accumulation steps = 1
06/22 06:57:10 PM:   Total optimization steps = 8060
Epoch:   0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]
D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Iteration-0(1):   6%|█████▍                                                                                       | 94/1612 [01:12<19:23,  1.31it/s]
Epoch:   0%|                                                                                                                  | 0/5 [01:12<?, ?it/s] 
Traceback (most recent call last):
  File "source/extractor_train.py", line 422, in <module>
    main()
  File "source/extractor_train.py", line 407, in main
    train(args, train_dataset, model, tokenizer, eval_dataset=eval_dataset, eval_fn=evaluation)
  File "source/extractor_train.py", line 105, in train
    stage1_outputs = model(batch_tensor[0], batch_tensor[1], batch_tensor[2], batch_tensor[3], batch_tensor[4],
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "E:\doc\projectPy\L-Defense_EFND_RE\source\extractor_model.py", line 109, in forward
    sents_embds[segment_indexes[_i]:segment_indexes[_i + 1]]).view(
KeyboardInterrupt

@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 1m 21.151s
[ 100] python source/extractor_train.py --model_name_or_path roberta-base --dataset_name RAWFC --output_dir ./runnings/step1_extraction_model_RAWFC --do_train --train_batch_size 1 --num_train_epochs 5 --eval_batch_size 32 --eval_steps 500 --max_seq_len 32 --gradient_accumulation_steps 1 --correlation_method mlp --learning_rate 1e-5 --cls_loss_weight 0.9
D:\04code\anaconda3\envs\efnd\lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EvidenceSelection were not initialized from the model checkpoint at roberta-base and are newly initialized: ['corre_fc1.bias', 'corre
_fc1.weight', 'corre_fc2.bias', 'corre_fc2.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight', 'fc4.bias', 'fc4.we
ight', 'predict_fc0.bias', 'predict_fc0.weight', 'predict_fc1.bias', 'predict_fc1.weight', 'predict_fc2.bias', 'predict_fc2.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1612/1612 [00:00<00:00, 23637.07it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 26221.77it/s] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 24998.09it/s] 
D:\04code\anaconda3\envs\efnd\lib\site-packages\transformers\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will 
be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/22 07:01:56 PM: ***** Running training *****
06/22 07:01:56 PM:   Num examples = 1612
06/22 07:01:56 PM:   Num Epochs = 5
06/22 07:01:56 PM:   Gradient Accumulation steps = 1
06/22 07:01:56 PM:   Total optimization steps = 8060
Epoch:   0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch s
ize and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Iteration-0(1):   3%|██▍                                                                                          | 43/1612 [00:34<21:11,  1.23it/s]
Epoch:   0%|                                                                                                                  | 0/5 [00:34<?, ?it/s] 
Traceback (most recent call last):
  File "source/extractor_train.py", line 422, in <module>
    main()
  File "source/extractor_train.py", line 407, in main
    train(args, train_dataset, model, tokenizer, eval_dataset=eval_dataset, eval_fn=evaluation)
  File "source/extractor_train.py", line 110, in train
    loss.backward()
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

PowerShell 7.4.2

   A new PowerShell stable release is available: v7.4.3 
   Upgrade now, or check out the release page at:       
     https://aka.ms/PowerShell-Release?tag=v7.4.3       

Loading personal and system profiles took 2579ms.
@26921 ➜ E:\..\L-Defense_EFND_RE ( base 3.11.5) git(master) 
[ 100] conda activate efnd   
@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 226ms
[ 100] ls

        Directory: E:\doc\projectPy\L-Defense_EFND_RE


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d----         2024/6/17     21:19                  __pycache__
d----         2024/6/22     18:21                  .idea
d----         2024/6/17     10:31                  conf
d----         2024/6/17     14:40                  dataset
d----         2024/6/17     21:20                  runnings
d----         2024/6/22     18:54                  source
-a---         2024/6/17     21:52            175   .gitignore
-a---         2024/6/17     22:07            340 󰉢  environment.yml
-a---         2024/6/22     17:30           8392   help.py
-a---         2024/6/17     21:59             79 󰈙  requirements.txt
-a---         2024/6/18     14:50           4148   roberta.md

@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 
[ 100] python source/extractor_train.py --model_name_or_path roberta-base --dataset_name RAWFC --output_dir ./runnings/step1_extraction_model_RAWFC --do_train --train_batch_size 1 --num_train_epochs 5 --eval_batch_size 32 --eval_steps 500 --max_seq_len 32 --gradient_accumulation_steps 1 --correlation_method mlp --learning_rate 1e-5 --cls_loss_weight 0.9
D:\04code\anaconda3\envs\efnd\lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EvidenceSelection were not initialized from the model checkpoint at roberta-base and are newly initialized: ['corre_fc1.bias', 'corre
_fc1.weight', 'corre_fc2.bias', 'corre_fc2.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight', 'fc4.bias', 'fc4.we
ight', 'predict_fc0.bias', 'predict_fc0.weight', 'predict_fc1.bias', 'predict_fc1.weight', 'predict_fc2.bias', 'predict_fc2.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1612/1612 [00:00<00:00, 25828.65it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 24599.30it/s] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 28577.39it/s] 
D:\04code\anaconda3\envs\efnd\lib\site-packages\transformers\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will 
be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/22 06:57:10 PM: ***** Running training *****
06/22 06:57:10 PM:   Num examples = 1612
06/22 06:57:10 PM:   Num Epochs = 5
06/22 06:57:10 PM:   Gradient Accumulation steps = 1
06/22 06:57:10 PM:   Total optimization steps = 8060
Epoch:   0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch s
ize and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Iteration-0(1):   6%|█████▍                                                                                       | 94/1612 [01:12<19:23,  1.31it/s]
Epoch:   0%|                                                                                                                  | 0/5 [01:12<?, ?it/s] 
Traceback (most recent call last):
  File "source/extractor_train.py", line 422, in <module>
    main()
  File "source/extractor_train.py", line 407, in main
    train(args, train_dataset, model, tokenizer, eval_dataset=eval_dataset, eval_fn=evaluation)
  File "source/extractor_train.py", line 105, in train
    stage1_outputs = model(batch_tensor[0], batch_tensor[1], batch_tensor[2], batch_tensor[3], batch_tensor[4],
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "E:\doc\projectPy\L-Defense_EFND_RE\source\extractor_model.py", line 109, in forward
    sents_embds[segment_indexes[_i]:segment_indexes[_i + 1]]).view(
KeyboardInterrupt
@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 1m 21.151s
[ 100] python source/extractor_train.py --model_name_or_path roberta-base --dataset_name RAWFC --output_dir ./runnings/step1_extraction_model_RAWFC --do_train --train_batch_size 1 --num_train_epochs 5 --eval_batch_size 32 --eval_steps 500 --max_seq_len 32 --gradient_accumulation_steps 1 --correlation_method mlp --learning_rate 1e-5 --cls_loss_weight 0.9
D:\04code\anaconda3\envs\efnd\lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EvidenceSelection were not initialized from the model checkpoint at roberta-base and are newly initialized: ['corre_fc1.bias', 'corre
_fc1.weight', 'corre_fc2.bias', 'corre_fc2.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight', 'fc4.bias', 'fc4.we
ight', 'predict_fc0.bias', 'predict_fc0.weight', 'predict_fc1.bias', 'predict_fc1.weight', 'predict_fc2.bias', 'predict_fc2.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1612/1612 [00:00<00:00, 23637.07it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 26221.77it/s] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 24998.09it/s] 
D:\04code\anaconda3\envs\efnd\lib\site-packages\transformers\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will 
be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/22 07:01:56 PM: ***** Running training *****
06/22 07:01:56 PM:   Num examples = 1612
06/22 07:01:56 PM:   Num Epochs = 5
06/22 07:01:56 PM:   Gradient Accumulation steps = 1
06/22 07:01:56 PM:   Total optimization steps = 8060
Epoch:   0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch s
ize and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Iteration-0(1):   3%|██▍                                                                                          | 43/1612 [00:34<21:11,  1.23it/s]
Epoch:   0%|                                                                                                                  | 0/5 [00:34<?, ?it/s] 
Traceback (most recent call last):
  File "source/extractor_train.py", line 422, in <module>
    main()
  File "source/extractor_train.py", line 407, in main
    train(args, train_dataset, model, tokenizer, eval_dataset=eval_dataset, eval_fn=evaluation)
  File "source/extractor_train.py", line 110, in train
    loss.backward()
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master) 40.72s
[ 100] python source/extractor_train.py --model_name_or_path roberta-base --dataset_name RAWFC --output_dir ./runnings/step1_extraction_model_RAWFC --do_train --train_batch_size 2 --num_train_epochs 5 --eval_batch_size 32 --eval_steps 500 --max_seq_len 32 --gradient_accumulation_steps 1 --correlation_method mlp --learning_rate 1e-5 --cls_loss_weight 0.9
D:\04code\anaconda3\envs\efnd\lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EvidenceSelection were not initialized from the model checkpoint at roberta-base and are newly initialized: ['corre_fc1.bias', 'corre
_fc1.weight', 'corre_fc2.bias', 'corre_fc2.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight', 'fc4.bias', 'fc4.we
ight', 'predict_fc0.bias', 'predict_fc0.weight', 'predict_fc1.bias', 'predict_fc1.weight', 'predict_fc2.bias', 'predict_fc2.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1612/1612 [00:00<00:00, 24818.64it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 28607.60it/s] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 26409.17it/s] 
D:\04code\anaconda3\envs\efnd\lib\site-packages\transformers\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will 
be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/22 07:02:58 PM: ***** Running training *****
06/22 07:02:58 PM:   Num examples = 1612
Epoch:   0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nnefnd\lib\site-packages\torch\nn\functional.py:2919' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                    06/22 07:43:58 PM: Ep[0] Loss: 1.0029     KL-Loss: 0.1656         CLS_loss: 1.0959                                | 99/806 [40:12<7:07:24, 36.27s/it]

Iteration-0(1):  18%|████████████████▊                                                                          | 149/806 [53:25<2:21:28, 12.92s/it]06/22 08:10:53 PM: Ep[0] Loss: 1.0077        KL-Loss: 0.1706atio CLS_loss: 1.1007█████████████████▉                                                                   | 199/806 [1:07:35<2:22:54, 14.13s/it]
                                                                                                                                                    06/22 08:39:28 PM: Ep[0] Loss: 1.0010        KL-Loss: 0.0037atio CLS_loss: 1.1119█████████████████████████████                                                        | 299/806 [1:36:11<2:04:48, 14.77s/it]
                                                                                                                                                    06/22 09:06:34 PM: Ep[0] Loss: 1.0181        KL-Loss: 0.3507atio CLS_loss: 1.0922████████████████████████████████████████                                             | 399/806 [2:03:28<1:49:43, 16.18s/it]
                                                                                                                                                    06/22 09:33:59 PM: Ep[0] Loss: 0.9881        KL-Loss: 0.0002atio CLS_loss: 1.0979███████████████████████████████████████████████████                                  | 499/806 [2:30:40<1:30:48, 17.75s/it]
06/22 09:33:59 PM: ***** Running evaluation at 500*****
06/22 09:33:59 PM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:33<03:22, 33.79s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [00:52<02:03, 24.71s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [02:55<04:39, 69.77s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [03:15<02:29, 49.96s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [03:31<01:15, 37.77s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [06:39<01:28, 88.87s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [06:40<00:00, 57.26s/it]
D:\04code\anaconda3\envs\efnd\lib\site-packages\sklearn\metrics\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [06:40<00:00, 60.30s/it] 
  _warn_prf(average, modifier, msg_start, len(result))
06/22 09:40:40 PM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
                                                                                                                                                    D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn
\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                    06/22 09:55:50 PM: Ep[0] Loss: 1.0031        KL-Loss: 0.3090atio CLS_loss: 1.0802███████████████████████████████████████████████████████████████▋                       | 599/806 [2:52:43<31:33,  9.15s/it]
                                                                                                                                                    06/22 10:11:55 PM: Ep[0] Loss: 1.0105        KL-Loss: 0.1735atio CLS_loss: 1.1035██████████████████████████████████████████████████████████████████████████▉            | 699/806 [3:08:47<16:15,  9.12s/it]
                                                                                                                                                    06/22 10:28:19 PM: Ep[0] Loss: 1.0126        KL-Loss: 0.4205atio CLS_loss: 1.0784██████████████████████████████████████████████████████████████████████████████████████▏| 799/806 [3:25:10<01:05,  9.41s/it]
Iteration-0(1): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 806/806 [3:26:19<00:00, 15.36s/it]
Epoch:  20%|███████████████████▌                                                                              | 1/5 [3:26:19<13:45:19, 12379.76s/it]                                                       06/22 10:47:32 PM: Ep[1] Loss: 0.9605█▉   KL-Loss: 0.0001         CLS_loss: 1.0672                                                                                       | 93/806 [18:04<1:55:18,  9.70s/it] 
                                                                                                                                                                                                           06/22 11:02:57 PM: Ep[1] Loss: 1.0021████ KL-Loss: 0.3420         CLS_loss: 1.0754                                                                                      | 193/806 [33:30<1:43:23, 10.12s/it] 
06/22 11:02:57 PM: ***** Running evaluation at 1000*****
06/22 11:02:57 PM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:21<02:11, 21.89s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [00:40<01:38, 19.79s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [02:38<04:18, 64.70s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [02:57<02:20, 46.88s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [03:14<01:11, 35.81s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [06:22<01:27, 87.72s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [06:24<00:00, 54.87s/it]
06/22 11:09:22 PM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [06:24<00:00, 59.53s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/22 11:24:46 PM: Ep[1] Loss: 0.9661████ KL-Loss: 0.3165████████ CLS_loss: 1.0382                                                                                      | 293/806 [55:19<1:20:02,  9.36s/it] 
                                                                                                                                                                                                           06/22 11:41:15 PM: Ep[1] Loss: 0.8960████ KL-Loss: 0.1771████████ CLS_loss: 0.9759██████████▏                                                                         | 393/806 [1:11:47<1:00:56,  8.85s/it] 
                                                                                                                                                                                                           06/22 11:58:20 PM: Ep[1] Loss: 1.0955████ KL-Loss: 0.4090████████ CLS_loss: 1.1718█████████████████████████████▎                                                        | 493/806 [1:28:51<51:46,  9.92s/it] 
                                                                                                                                                                                                           06/23 12:15:13 AM: Ep[1] Loss: 0.9706████ KL-Loss: 0.1263████████ CLS_loss: 1.0644███████████████████████████████████████████████▍                                      | 593/806 [1:45:42<37:17, 10.50s/it] 
                                                                                                                                                                                                           06/23 12:31:58 AM: Ep[1] Loss: 1.0942████ KL-Loss: 0.4339████████ CLS_loss: 1.1676█████████████████████████████████████████████████████████████████▌                    | 693/806 [2:02:29<20:48, 11.05s/it] 
06/23 12:31:58 AM: ***** Running evaluation at 1500*****
06/23 12:31:58 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:21<02:10, 21.67s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [00:42<01:46, 21.21s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [02:32<04:07, 61.78s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [02:54<02:17, 45.86s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [03:14<01:13, 36.78s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [06:12<01:24, 84.78s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [06:26<00:00, 55.18s/it]
                                                                                                                                                                                                           D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/23 12:55:39 AM: Ep[1] Loss: 1.0246████ KL-Loss: 0.3377████████ CLS_loss: 1.1010███████████████████████████████████████████████████████████████████████████████████▋  | 793/806 [2:26:09<02:26, 11.27s/it] 
Iteration-1(1): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [2:28:23<00:00, 11.05s/it]
Epoch:  40%|███████████████████████████████████████▌                                                           | 2/5 [5:54:43<8:36:45, 10335.14s/it]███████████████████| 806/806 [2:28:23<00:00, 10.07s/it]06/23 01:13:53 AM: Ep[2] Loss: 0.8775▊    KL-Loss: 0.2236         CLS_loss: 0.9501                                                                                       | 87/806 [15:59<2:23:13, 11.95s/it] 
                                                                                                                                                                                                           06/23 01:33:24 AM: Ep[2] Loss: 0.9537████ KL-Loss: 0.1738         CLS_loss: 1.0404                                                                                      | 187/806 [35:33<2:03:47, 12.00s/it] 
                                                                                                                                                                                                           06/23 01:53:25 AM: Ep[2] Loss: 0.7986████ KL-Loss: 0.1266████████ CLS_loss: 0.8733                                                                                      | 287/806 [55:33<1:46:28, 12.31s/it] 
                                                                                                                                                                                                           06/23 02:13:14 AM: Ep[2] Loss: 0.7872████ KL-Loss: 0.1315████████ CLS_loss: 0.8601█████████▏                                                                          | 387/806 [1:15:19<1:23:50, 12.01s/it] 
06/23 02:13:14 AM: ***** Running evaluation at 2000*****
06/23 02:13:14 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:43<04:18, 43.03s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [01:25<03:33, 42.73s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [03:18<04:59, 74.88s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [04:01<03:07, 62.35s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [04:43<01:50, 55.06s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [07:26<01:31, 91.71s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:44<00:00, 66.32s/it]
06/23 02:20:58 AM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:44<00:00, 67.43s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/23 02:41:14 AM: Ep[2] Loss: 0.7793████ KL-Loss: 0.1117████████ CLS_loss: 0.8535███████████████████████████                                                         | 487/806 [1:43:22<1:04:02, 12.05s/it] 
                                                                                                                                                                                                           06/23 03:00:51 AM: Ep[2] Loss: 0.7707████ KL-Loss: 0.1063████████ CLS_loss: 0.8445██████████████████████████████████████████████▎                                       | 587/806 [2:02:58<38:15, 10.48s/it] 
                                                                                                                                                                                                           06/23 03:19:38 AM: Ep[2] Loss: 0.9434████ KL-Loss: 0.1744████████ CLS_loss: 1.0289████████████████████████████████████████████████████████████████▍                     | 687/806 [2:21:44<22:29, 11.34s/it] 
                                                                                                                                                                                                           06/23 03:39:26 AM: Ep[2] Loss: 0.9687████ KL-Loss: 0.1380████████ CLS_loss: 1.0610██████████████████████████████████████████████████████████████████████████████████▌   | 787/806 [2:41:34<03:52, 12.25s/it] 
Iteration-2(1): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [2:45:08<00:00, 12.29s/it]
Epoch:  60%|███████████████████████████████████████████████████████████▍                                       | 3/5 [8:39:52<5:38:00, 10140.29s/it]███████████████████| 806/806 [2:45:08<00:00, 12.37s/it]06/23 03:58:41 AM: Ep[3] Loss: 0.8903     KL-Loss: 0.0140         CLS_loss: 0.9876                                                                                       | 81/806 [15:39<2:36:23, 12.94s/it] 
06/23 03:58:41 AM: ***** Running evaluation at 2500*****
06/23 03:58:41 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:43<04:18, 43.00s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [01:25<03:33, 42.75s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [03:18<04:59, 74.87s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [04:02<03:07, 62.42s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [04:44<01:50, 55.07s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [07:27<01:31, 91.85s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:44<00:00, 66.41s/it]
06/23 04:06:26 AM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:44<00:00, 67.56s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/23 04:26:01 AM: Ep[3] Loss: 0.8366████ KL-Loss: 0.0162         CLS_loss: 0.9278                                                                                      | 181/806 [42:58<2:01:12, 11.64s/it] 
                                                                                                                                                                                                           06/23 04:45:11 AM: Ep[3] Loss: 1.2107████ KL-Loss: 0.1847████████ CLS_loss: 1.3247                                                                                    | 281/806 [1:02:06<1:47:18, 12.26s/it] 
                                                                                                                                                                                                           06/23 05:04:10 AM: Ep[3] Loss: 0.9325████ KL-Loss: 0.3207████████ CLS_loss: 1.0004████████                                                                            | 381/806 [1:21:07<1:17:14, 10.90s/it] 
                                                                                                                                                                                                           06/23 05:23:17 AM: Ep[3] Loss: 0.6879████ KL-Loss: 0.0983████████ CLS_loss: 0.7534█████████████████████████▉                                                          | 481/806 [1:40:16<1:09:43, 12.87s/it] 
                                                                                                                                                                                                           06/23 05:42:44 AM: Ep[3] Loss: 0.8123████ KL-Loss: 0.2287████████ CLS_loss: 0.8771█████████████████████████████████████████████▏                                        | 581/806 [1:59:43<42:16, 11.28s/it] 
06/23 05:42:44 AM: ***** Running evaluation at 3000*****
06/23 05:42:44 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:40<04:03, 40.50s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [01:20<03:20, 40.13s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [03:12<04:51, 72.79s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [03:52<03:00, 60.06s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [04:32<01:45, 52.66s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [07:09<01:28, 88.33s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:27<00:00, 63.88s/it]
                                                                                                                                                                                                           D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/23 06:09:28 AM: Ep[3] Loss: 1.0096████ KL-Loss: 0.2622████████ CLS_loss: 1.0926███████████████████████████████████████████████████████████████▎                      | 681/806 [2:26:26<20:33,  9.87s/it] 
                                                                                                                                                                                                           06/23 06:28:08 AM: Ep[3] Loss: 0.8743████ KL-Loss: 0.0005████████ CLS_loss: 0.9714█████████████████████████████████████████████████████████████████████████████████▍    | 781/806 [2:45:07<04:35, 11.00s/it] 
Iteration-3(1): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [2:49:45<00:00, 12.64s/it]
Epoch:  80%|██████████████████████████████████████████████████████████████████████████████▍                   | 4/5 [11:29:37<2:49:18, 10158.17s/it]███████████████████| 806/806 [2:49:45<00:00, 11.63s/it]06/23 06:47:21 AM: Ep[4] Loss: 1.0673     KL-Loss: 0.2516         CLS_loss: 1.1579                                                                                       | 75/806 [14:35<2:29:05, 12.24s/it] 
                                                                                                                                                                                                           06/23 07:06:40 AM: Ep[4] Loss: 0.9853████ KL-Loss: 0.3250         CLS_loss: 1.0587                                                                                      | 175/806 [33:54<1:57:37, 11.18s/it] 
                                                                                                                                                                                                           06/23 07:25:34 AM: Ep[4] Loss: 0.6978████ KL-Loss: 0.1038████████ CLS_loss: 0.7638                                                                                      | 275/806 [52:44<1:33:41, 10.59s/it] 
06/23 07:25:34 AM: ***** Running evaluation at 3500*****
06/23 07:25:34 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:40<04:03, 40.58s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [01:20<03:20, 40.14s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [03:12<04:51, 72.93s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [03:52<03:00, 60.11s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [04:32<01:45, 52.69s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [07:10<01:28, 88.50s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:27<00:00, 63.97s/it]
06/23 07:33:02 AM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:27<00:00, 65.26s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
                                                                                                                                                                                                           06/23 07:52:19 AM: Ep[4] Loss: 1.0343████ KL-Loss: 0.1542████████ CLS_loss: 1.1321██████▉                                                                             | 375/806 [1:19:32<1:29:18, 12.43s/it] 
                                                                                                                                                                                                           06/23 08:11:36 AM: Ep[4] Loss: 0.7664████ KL-Loss: 0.2109████████ CLS_loss: 0.8281██████████████████████████                                                            | 475/806 [1:38:49<58:14, 10.56s/it] 
                                                                                                                                                                                                           06/23 08:30:43 AM: Ep[4] Loss: 1.0997████ KL-Loss: 0.3192████████ CLS_loss: 1.1864████████████████████████████████████████████▏                                         | 575/806 [1:57:57<43:33, 11.31s/it] 
                                                                                                                                                                                                           06/23 08:49:36 AM: Ep[4] Loss: 1.0027████ KL-Loss: 0.2296████████ CLS_loss: 1.0886██████████████████████████████████████████████████████████████▎                       | 675/806 [2:16:49<24:42, 11.32s/it] 
                                                                                                                                                                                                           06/23 09:08:40 AM: Ep[4] Loss: 0.9011████ KL-Loss: 0.0226████████ CLS_loss: 0.9987████████████████████████████████████████████████████████████████████████████████▍     | 775/806 [2:35:52<05:46, 11.18s/it] 
06/23 09:08:40 AM: ***** Running evaluation at 4000*****
06/23 09:08:40 AM:   Num examples = 200
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 1/7 [00:40<04:02, 40.36s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 2/7 [01:20<03:20, 40.05s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 3/7 [03:11<04:50, 72.73s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 4/7 [03:52<02:59, 60.00s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.                                 | 5/7 [04:31<01:45, 52.63s/it] 
  warnings.warn(
                                                                                                                                                                                                           D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.██████████▋                      | 6/7 [07:09<01:28, 88.26s/it] 
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:26<00:00, 63.82s/it]
06/23 09:16:07 AM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:26<00:00, 65.11s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Iteration-4(1): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [2:49:17<00:00, 12.60s/it]
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [14:18:55<00:00, 10307.10s/it]███████████████████| 806/806 [2:49:17<00:00, 12.66s/it] 
06/23 09:21:54 AM: ***** Running evaluation at 4030*****
06/23 09:21:54 AM:   Num examples = 200
Evaluating:  14%|██████████████████████▎                                                                                                                                     | 1/7 [00:40<04:02, 40.46s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Evaluating:  29%|████████████████████████████████████████████▌                                                                                                               | 2/7 [01:20<03:20, 40.07s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Evaluating:  43%|██████████████████████████████████████████████████████████████████▊                                                                                         | 3/7 [03:13<04:53, 73.33s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 4/7 [03:53<03:01, 60.37s/it]D
:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 5/7 [04:33<01:45, 52.90s/it]D
  warnings.warn(
Evaluating:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 6/7 [07:11<01:28, 88.55s/it]D:\04code\anaconda3\envs\efnd\lib\site-packages\torch\nn\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:28<00:00, 64.06s/it]
06/23 09:29:22 AM: Saving model checkpoint to ./runnings/step1_extraction_model_RAWFC
@26921 ➜ E:\..\L-Defense_EFND_RE ( efnd 3.8.19) git(master14h 26m 30.771s
[ 100]

